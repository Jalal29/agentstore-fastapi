from fastapi import FastAPI, Query
from langchain.vectorstores import Chroma
from langchain.embeddings import SentenceTransformerEmbeddings
from typing import List
import os
import logging
from fastapi.middleware.cors import CORSMiddleware

app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allow all origins (for debugging)
    allow_credentials=True,
    allow_methods=["*"],  
    allow_headers=["*"],  
)


# ✅ Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

VECTOR_DB_PATH = "C:/Users/mdjal/OneDrive/Desktop/langchain/Resume_vector_db"

# ✅ Initialize the embeddings
embedding_model = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")

# ✅ Load existing vector database
if os.path.exists(VECTOR_DB_PATH):
    vector_db = Chroma(persist_directory=VECTOR_DB_PATH, embedding_function=embedding_model)
    retriever = vector_db.as_retriever()
    logger.info("Vector database loaded successfully.")
else:
    raise RuntimeError("Vector database not found. Ensure you have persisted it before running FastAPI.")

# ✅ Load the stored Chroma vector database at startup

from langchain_groq import ChatGroq
from langchain.schema import HumanMessage
groq_api_key='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'

llm = ChatGroq(model='qwen-2.5-32b',groq_api_key=groq_api_key)

def generate_explanation(job_description, resume_text, similarity_score):
    """Use LLM to explain why this resume matches the job description."""
    prompt = f"""
    Given the job description:
    {job_description}

    And this resume snippet:
    {resume_text[:1000]}

    This resume has a similarity score of {similarity_score:.4f}.
    
    Explain in simple terms why this resume is relevant in .
    """
    
    
    response = llm([HumanMessage(content=prompt)])
    return response.content


def summarize_explanation(job_description,explanation,):
    llm_prompt="""You are an expert career analyst. Given an explanation of a resume generated by a previous agent—which outlines why the resume matches the job description—your task is to provide a shortened summary of the candidate’s strengths and weaknesses in bullet points, strictly based on the explanation from the previous agent.

Strengths (Positive aspects): Highlight key skills, achievements, and experiences mentioned in the explanation.
Weaknesses (Negative aspects): Identify areas for improvement explicitly stated in the explanation. Do not infer missing skills unless they are explicitly mentioned as a gap in the explanation.
Ensure that the response follows this format:

Example Output:

Strengths:

Strong expertise in relevant industry skills and technologies.
Proven experience in handling key responsibilities efficiently.
Demonstrated leadership, teamwork, or problem-solving capabilities.
Consistent career growth and professional achievements.
Weaknesses:

Areas of improvement explicitly mentioned in the explanation.
Any stated concerns about experience gaps or required credentials.
Career trajectory inconsistencies or frequent job changes.
Do not infer missing skills unless explicitly mentioned as a requirement in the explanation.
"""
    prompt = f"""
    Given the job description:
    {job_description}
    This is the full resume explanation  from previous agent:
    {explanation}

    {llm_prompt}
    """
    response = llm([HumanMessage(content=prompt)])
    return response.content


@app.get("/")
def home():
    return {"message": "Welcome to the Resume ranker API"}

@app.get("/search/")
def search(job_description: str = Query(..., description="Enter your query text"),
    option: str = Query(..., description="Resume type filter"), k: int = 5):
    """
    Search for the most similar documents in the vector database.
    """
    try:
     
        print(1)
        print(option)
        a=dict()
        
        if option=="SoftwareEngineer":
            a["resume_category"]='SE'
        if option=="DataAnalyst":
            a["resume_category"]='DataAnalyst'

        print(a)
        
        ##taskkill /IM python.exe /F
        if a:
            results = retriever.vectorstore.similarity_search_with_score(job_description, k=5, filter=a)
        else:
            results = retriever.vectorstore.similarity_search_with_score(job_description, k=5)
        print(results)
          # ✅ Using retriever instead of direct vector_db call
        unique_results = []
        seen_texts = set()

        for doc, score in results:
            
            if doc.metadata['resume_name'] not in seen_texts:
                unique_results.append((doc, score))
                seen_texts.add(doc.metadata['resume_name'])

        # # Take only the first 5 unique results
        results = unique_results[:5]
        explanation_list=[]
        for r in results:
            explanation = generate_explanation(job_description, r[0].page_content, r[1])
            explanation_list.append(explanation)

        summary_list=[]
        for i  in range(len(results)):
            summary = summarize_explanation(job_description, explanation_list[i],)
            summary_list.append(summary)
        
            
        response = [
            {
                "Score": results[i][1],
                "explain": explanation_list[i],
                "summary": summary_list[i],
                "file_path": "http://localhost:7000/"+ results[i][0].metadata['resume_name'],
            }
            for i in range(len(results))
        ]
     
        
        return { "results": response}
    except Exception as e:
        logger.error(f"Search failed: {str(e)}")
        return {"error": "An internal error occurred. Please try again later."}
